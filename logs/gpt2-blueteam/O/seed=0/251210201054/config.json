{
    "max_steps": 500,
    "n_tests": 300,
    "victim_model": "llava",
    "ppo_config": {
        "exp_name": "ppo_blue_team",
        "seed": 0,
        "log_with": "tensorboard",
        "task_name": "gpt2-blueteam",
        "model_name": "sshleifer/tiny-gpt2",
        "query_dataset": "SafetyBench",
        "reward_model": "offline",
        "remove_unused_columns": true,
        "tracker_kwargs": {},
        "accelerator_kwargs": {
            "project_dir": "/media/ssd5/hxy/defense/BlueSuffix/logs/gpt2-blueteam/O/seed=0/251210201054"
        },
        "project_kwargs": {},
        "tracker_project_name": "trl",
        "push_to_hub_if_best_kwargs": {},
        "steps": 20000,
        "learning_rate": 3e-05,
        "adap_kl_ctrl": false,
        "init_kl_coef": 0.001,
        "kl_penalty": "kl",
        "target": 6,
        "horizon": 10000,
        "gamma": 1,
        "lam": 0.95,
        "cliprange": 0.2,
        "cliprange_value": 0.2,
        "vf_coef": 0.1,
        "batch_size": 1,
        "forward_batch_size": null,
        "mini_batch_size": 1,
        "gradient_accumulation_steps": 1,
        "world_size": null,
        "ppo_epochs": 1,
        "max_grad_norm": null,
        "optimize_cuda_cache": null,
        "optimize_device_cache": false,
        "early_stopping": false,
        "target_kl": 1,
        "compare_steps": 1,
        "ratio_threshold": 10.0,
        "use_score_scaling": false,
        "use_score_norm": false,
        "score_clip": null,
        "whiten_rewards": false,
        "gradient_checkpointing": false,
        "is_encoder_decoder": null,
        "is_peft_model": null,
        "backward_batch_size": 1,
        "global_backward_batch_size": null,
        "global_batch_size": null
    },
    "use_seq2seq": false,
    "use_peft": false,
    "peft_config": {
        "peft_type": "LORA",
        "auto_mapping": null,
        "base_model_name_or_path": null,
        "revision": null,
        "task_type": "CAUSAL_LM",
        "inference_mode": false,
        "r": 16,
        "target_modules": null,
        "lora_alpha": 16,
        "lora_dropout": 0.0,
        "fan_in_fan_out": false,
        "bias": "none",
        "use_rslora": false,
        "modules_to_save": null,
        "init_lora_weights": true,
        "layers_to_transform": null,
        "layers_pattern": null,
        "rank_pattern": {},
        "alpha_pattern": {},
        "megatron_config": null,
        "megatron_core": "megatron.core",
        "loftq_config": {}
    },
    "save": true,
    "test_after_train": true,
    "k": 10,
    "full_test": false
}